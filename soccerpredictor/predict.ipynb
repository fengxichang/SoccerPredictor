{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from util.constants import *\n",
    "from util.common import *\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import compute_class_weight\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接sqlite3\n",
    "conn = sqlite3.connect(f\"file:../data/db/soccer.db?mode=rw\", uri=True) \n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义常量\n",
    "_seed = 99\n",
    "_timesteps = 10\n",
    "_epochs = 2\n",
    "seasons = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
    "_features = FEATURES_COMMON + FEATURES_WD\n",
    "_scalers = {f: MinMaxScaler(feature_range=(0, 1)) for f in FEATURES_TO_SCALE if f in _features}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_teams_names() -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Queries teams' names from db.\n",
    "        Ordered by team name.\n",
    "\n",
    "        :return: Dataframe of all teams names in db.\n",
    "        \"\"\"\n",
    "        return pd.read_sql(\"\"\"\n",
    "            SELECT t.name  \n",
    "            FROM Teams t\n",
    "            ORDER BY t.name\n",
    "            \"\"\", conn)\n",
    "\n",
    "# 查询fixtures数据\n",
    "def query_fixtures_data(seasons: List[int]) -> pd.DataFrame:\n",
    "        df = pd.read_sql(\"\"\"\n",
    "            SELECT f.id, f.date, f.season, f.league, \n",
    "                   t1.name AS home, t2.name AS away, f.home_goals, f.away_goals, \n",
    "                   f.oddsDC_1X AS home_odds_wd, f.oddsDC_X2 AS away_odds_wd,\n",
    "                   ts1.rating AS home_rating, ts2.rating AS away_rating,\n",
    "                   ts1.errors AS home_errors, ts2.errors AS away_errors, \n",
    "                   ts1.red_cards AS home_red_cards, ts2.red_cards AS away_red_cards,\n",
    "                   ts1.shots AS home_shots, ts2.shots AS away_shots,\n",
    "                   ts1.opponent_goals AS home_opponent_goals, ts1.opponent_shots AS home_opponent_shots,\n",
    "                   ts1.opponent_rating AS home_opponent_rating, ts2.opponent_rating AS away_opponent_rating,\n",
    "                   ts2.opponent_goals As away_opponent_goals, ts2.opponent_shots AS away_opponent_shots \n",
    "            FROM Fixtures f\n",
    "            JOIN Teams t1 ON f.homeTeamID = t1.id\n",
    "            JOIN Teams t2 ON f.awayTeamID = t2.id\n",
    "            JOIN TeamStats ts1 ON f.homeStatsID = ts1.id\n",
    "            JOIN TeamStats ts2 ON f.awayStatsID = ts2.id\n",
    "            WHERE f.season IN ({})\n",
    "            ORDER BY f.date, f.id\n",
    "            \"\"\".format(\",\".join(\"?\" * len(seasons))),\n",
    "                         conn, params=seasons)\n",
    "\n",
    "        return df\n",
    "\n",
    "def query_team_data(seasons: List[int], params: Tuple[Any, ...]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Queries fixtures data for a single team within given seasons.\n",
    "\n",
    "        :param seasons: Seasons to query.\n",
    "        :param params: Params for query.\n",
    "        :return: Team fixtures df.\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(\"\"\"\n",
    "            SELECT f.id, f.date, f.season, f.league, f.homeTeamID, f.awayTeamID,\n",
    "                   t1.name AS home, t2.name AS away, f.home_goals, f.away_goals, f.winner,\n",
    "                   ts.rating, ts.goals, ts.errors, ts.red_cards, ts.shots, f.oddsDC_1X, f.oddsDC_X2,\n",
    "                   ts.opponent_goals, ts.opponent_shots, ts.opponent_rating \n",
    "            FROM TeamStats ts\n",
    "            JOIN Fixtures f ON f.id = ts.fixtureID \n",
    "            JOIN Teams t1 ON f.homeTeamID = t1.id\n",
    "            JOIN Teams t2 ON f.awayTeamID = t2.id\n",
    "            WHERE ts.teamID = ? AND (f.homeTeamID = ? OR f.awayTeamID = ?) AND\n",
    "                  f.season IN ({})\n",
    "            ORDER BY f.date, f.id\n",
    "            \"\"\".format(\",\".join(\"?\" * len(seasons))),\n",
    "            conn, params=params)\n",
    "\n",
    "        return df\n",
    "\n",
    "def query_teams_ids_names_tuples() -> Dict[str, int]:\n",
    "        df = pd.read_sql(\"\"\"\n",
    "            SELECT t.id, t.name\n",
    "            FROM Teams t\n",
    "            ORDER BY t.id\n",
    "            \"\"\", conn)\n",
    "\n",
    "        return dict(zip(df[\"name\"], df[\"id\"]))\n",
    "\n",
    "# 检查缺失列\n",
    "def _check_missing_columns(df: pd.DataFrame) -> None:\n",
    "        if any([c not in df.columns for c in REQUIRED_COLUMNS]):\n",
    "            raise ValueError(\"Missing columns in dataset.\"\n",
    "                             f\"Columns: {df.columns}\"\n",
    "                             f\"Required: {REQUIRED_COLUMNS}\")\n",
    "\n",
    "# 检查缺失值\n",
    "def _check_nan_values(df: pd.DataFrame, teams_fixtures_ids: Dict[str, List[int]]) -> pd.DataFrame:\n",
    "        teams_fixtures_lastid = {k: v[-1] for k, v in teams_fixtures_ids.items()}\n",
    "\n",
    "        mask_lastids = df[\"id\"].isin(set(teams_fixtures_lastid.values()))\n",
    "        df_except_lastid = df[~mask_lastids].copy()\n",
    "        df_lastid_only = df[mask_lastids][REQUIRED_TARGET_COLUMNS]\n",
    "\n",
    "        # Check missing values in data except for teams' last matches (targets)\n",
    "        if df_except_lastid.isna().any().any():\n",
    "            print(\"Missing values found in the dataset:\")\n",
    "            print(df_except_lastid.isna().sum())\n",
    "\n",
    "            # If there are still nans then there are another missing data\n",
    "            if df_except_lastid.isna().any().any():\n",
    "                print(\"There are still missing values in the dataset:\")\n",
    "                print(df_except_lastid.isna().sum())\n",
    "                raise ValueError(\"Dataset contains some missing values.\")\n",
    "\n",
    "        # Check if last matches (targets) contain any nans for required columns\n",
    "        if df_lastid_only.isna().any().any():\n",
    "            print(df_lastid_only.isna().sum())\n",
    "            raise ValueError(\"Dataset contains some missing target values.\")\n",
    "\n",
    "        return df    \n",
    "\n",
    "\n",
    "def _mask_out_dataset(df: pd.DataFrame, _train_fixtures_ids, _test_fixtures_ids, _predict_fixtures_ids) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        mask_train = df[\"id\"].isin(set(chain(*_train_fixtures_ids.values())))\n",
    "        mask_test = df[\"id\"].isin(set(chain(*_test_fixtures_ids.values())))\n",
    "        mask_predict = df[\"id\"].isin(set(chain(*_predict_fixtures_ids.values())))\n",
    "\n",
    "        df_train = df[mask_train & ~mask_test & ~mask_predict].copy()\n",
    "        df_test = df[mask_test & ~mask_predict].copy()\n",
    "        df_predict = df[mask_predict].copy()\n",
    "\n",
    "        print(f\"Train dataset total samples: {len(df_train)}\")\n",
    "        print(f\"Test dataset total samples: {len(df_test)}\")\n",
    "        print(f\"Predict dataset total samples: {len(df_predict)}\")\n",
    "\n",
    "        # Additional argument checking which can be done only after computing individual datasets.\n",
    "        # Check if datasets are empty (there is a check during parsing arguments that test/predict\n",
    "        # split samples must be at least 1, but some dataset may become empty in the end due to some\n",
    "        # restrictions and filtering).\n",
    "        emsg = \"Maybe try to specify the split of test and/or prediction samples more reasonably?\"\n",
    "        if df_train.empty:\n",
    "            raise ValueError(f\"Train dataset is empty. {emsg}\")\n",
    "        if df_test.empty:\n",
    "            raise ValueError(f\"Test dataset is empty. {emsg}\")\n",
    "        if df_predict.empty:\n",
    "            raise ValueError(f\"Predict dataset is empty. {emsg}\")\n",
    "        # Check if datasets are too large (e.g. predict dataset is larger than the rest, etc.)\n",
    "        if len(df_predict) > len(df_test) + len(df_train):\n",
    "            raise ValueError(f\"Number of samples in predict dataset is too large. {emsg}\")\n",
    "        if len(df_test) > len(df_train):\n",
    "            raise ValueError(f\"Number of samples in test dataset is too large. {emsg}\")\n",
    "\n",
    "        return df_train, df_test, df_predict\n",
    "\n",
    "\n",
    "def fit_scalers(df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Fits scalers on columns with the same name.\n",
    "        Features specified in FEATURES_TO_SCALE are fit on columns with both \"home_\" and \"away_\" prefixes.\n",
    "\n",
    "        :param df: Dataframe on which scalers should be fit.\n",
    "        \"\"\"\n",
    "        for feature, scaler in _scalers.items():\n",
    "            if feature == \"season\":\n",
    "                scaler.fit(df[\"season\"].unique().reshape(-1, 1))\n",
    "            elif feature in FEATURES_TO_SCALE:\n",
    "                values = np.concatenate((df[f\"home_{feature}\"].values, df[f\"away_{feature}\"].values))\n",
    "                scaler.fit(np.unique(values).reshape(-1, 1))\n",
    "            else:\n",
    "                scaler.fit(df[feature].unique().reshape(-1, 1))\n",
    "\n",
    "def get_fixtures_ids_from_df(df: pd.DataFrame, team: str) -> List[int]:\n",
    "    return df[(df[\"home\"] == team) | (df[\"away\"] == team)].loc[:, \"id\"].tolist()\n",
    "\n",
    "def _fit_teams_names_lencoder() -> LabelEncoder:\n",
    "        \"\"\"\n",
    "        Fits a LabelEncoder for teams names.\n",
    "        Queries teams names from db and fits encoder to map the names from strings to integers\n",
    "        starting from 0 (including empty string for possible unknown/missing team name).\n",
    "\n",
    "        :return: Fit LabelEncoder.\n",
    "        \"\"\"\n",
    "        df = query_teams_names()\n",
    "        values = [\"\"] + df[\"name\"].values.tolist()\n",
    "        return LabelEncoder().fit(values)\n",
    "\n",
    "def _transform_team_name(team_name: str) -> np.ndarray:\n",
    "        _teams_names_lenc = _fit_teams_names_lencoder()\n",
    "        return np.array(list(np.binary_repr(_teams_names_lenc.transform([team_name])[0],\n",
    "                                            width=len(_teams_names_lenc.classes_).bit_length())),\n",
    "                        dtype=int)\n",
    "\n",
    "\n",
    "def _scale_team_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for f in FEATURES_TO_LENC:\n",
    "            nonnull_index = ~df[f].isnull()\n",
    "            df.loc[nonnull_index, f] = df.loc[nonnull_index, f].apply(_transform_team_name)\n",
    "\n",
    "        for f, scaler in _scalers.items():\n",
    "            nonnull_index = ~df[f].isnull()\n",
    "            df.loc[nonnull_index, f] = scaler.transform(df.loc[nonnull_index, f].values.reshape(-1, 1))\n",
    "\n",
    "            # Also transform columns with \"future_\" prefix with the same scalers\n",
    "            futuref = f\"future_{f}\"\n",
    "            if futuref in df.columns:\n",
    "                nonnull_index = ~df[futuref].isnull()\n",
    "                df.loc[nonnull_index, futuref] = scaler.transform(df.loc[nonnull_index, futuref].values.reshape(-1, 1))\n",
    "\n",
    "        return df\n",
    "\n",
    "def load_and_process_team_data(dataset: Dataset, teamid: int, team_fixtures_idx: List[int]) -> pd.DataFrame:\n",
    "    df = query_team_data(seasons, params=(teamid, teamid, teamid, *seasons))\n",
    "\n",
    "    # Current win-or-draw target from team's POV\n",
    "    df[\"wd\"] = np.select(\n",
    "        condlist=[\n",
    "            (df[\"homeTeamID\"] == teamid) & (df[\"winner\"] == 1),\n",
    "            (df[\"homeTeamID\"] == teamid) & (df[\"winner\"] == 2),\n",
    "            (df[\"awayTeamID\"] == teamid) & (df[\"winner\"] == 2),\n",
    "            (df[\"awayTeamID\"] == teamid) & (df[\"winner\"] == 1),\n",
    "            df[\"winner\"] == 0\n",
    "        ],\n",
    "        choicelist=[\n",
    "            1,\n",
    "            0,\n",
    "            1,\n",
    "            0,\n",
    "            1\n",
    "        ],\n",
    "        default=np.nan\n",
    "    )\n",
    "    # Odds for wd from team's POV\n",
    "    df[\"odds_wd\"] = np.select(\n",
    "        condlist=[\n",
    "            df[\"homeTeamID\"] == teamid,\n",
    "            df[\"awayTeamID\"] == teamid\n",
    "        ],\n",
    "        choicelist=[\n",
    "            df[\"oddsDC_1X\"],\n",
    "            df[\"oddsDC_X2\"]\n",
    "        ],\n",
    "        default=np.nan\n",
    "    )\n",
    "    # `team` is just name of current team\n",
    "    df[\"team\"] = np.select(\n",
    "        condlist=[\n",
    "            df[\"homeTeamID\"] == teamid,\n",
    "            df[\"awayTeamID\"] == teamid\n",
    "        ],\n",
    "        choicelist=[\n",
    "            df[\"home\"],\n",
    "            df[\"away\"]\n",
    "        ],\n",
    "        default=np.nan\n",
    "    )\n",
    "    # `opponent` is the opposite team to `team`\n",
    "    df[\"opponent\"] = np.select(\n",
    "        condlist=[\n",
    "            df[\"homeTeamID\"] == teamid,\n",
    "            df[\"awayTeamID\"] == teamid\n",
    "        ],\n",
    "        choicelist=[\n",
    "            df[\"away\"],\n",
    "            df[\"home\"]\n",
    "        ],\n",
    "        default=np.nan\n",
    "    )\n",
    "    # Whether the current team plays as home = 1, or away = 0\n",
    "    df[\"ashome\"] = np.select(\n",
    "        condlist=[\n",
    "            df[\"homeTeamID\"] == teamid,\n",
    "            df[\"awayTeamID\"] == teamid,\n",
    "        ],\n",
    "        choicelist=[\n",
    "            1,\n",
    "            0\n",
    "        ],\n",
    "        default=np.nan\n",
    "    )\n",
    "    # Encode league: PL = 1, CH = 0\n",
    "    df[\"league\"] = np.select(\n",
    "        condlist=[\n",
    "            df[\"league\"] == \"PL\",\n",
    "            df[\"league\"] == \"CH\"\n",
    "        ],\n",
    "        choicelist=[\n",
    "            1,\n",
    "            0\n",
    "        ],\n",
    "        default=np.nan\n",
    "    )\n",
    "\n",
    "    # Create future values of features which are known in advance.\n",
    "    # They are created by shifting original values by one row.\n",
    "    df[\"future_season\"] = df[\"season\"].shift(-1)\n",
    "    df[\"future_league\"] = df[\"league\"].shift(-1)\n",
    "    df[\"future_ashome\"] = df[\"ashome\"].shift(-1)\n",
    "    df[\"future_opponent\"] = df[\"opponent\"].shift(-1)\n",
    "    df[\"future_wd\"] = df[\"wd\"].shift(-1)\n",
    "    df[\"future_odds_wd\"] = df[\"odds_wd\"].shift(-1)\n",
    "\n",
    "    # Dataframe needs to be filtered according to current dataset to contain only corresponding\n",
    "    # matches. Also index needs to be reset (to correctly access data for test and predict sets).\n",
    "    df = df.loc[df[\"id\"].isin(team_fixtures_idx)].copy()\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # If there are nans for train dataset, use ffill method to fill them.\n",
    "    # This can happen if team occurs only in train dataset and does not have more data in later\n",
    "    # seasons, thus it does not matter that we fill these nans with incorrect data as they wo not be\n",
    "    # used for testing anyway.\n",
    "    if dataset == Dataset.Train:\n",
    "        # Fill nan values of future opponent with empty string\n",
    "        df[\"future_opponent\"].fillna(\"\", inplace=True)\n",
    "        df.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "    df = _scale_team_data(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def align_fixtures_ids(df: pd.DataFrame,\n",
    "                       team: str,\n",
    "                       fixtures_ids: List[int],\n",
    "                       _timesteps: int) -> List[int]:\n",
    "    aligned_matches = df[(df[\"home\"] == team) | (df[\"away\"] == team)][-_timesteps:]\n",
    "    aligned_fixtures_ids = get_fixtures_ids_from_df(aligned_matches, team)\n",
    "\n",
    "    return aligned_fixtures_ids + fixtures_ids\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Input, concatenate, Dense, LSTM, Layer\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "class SPNetwork:\n",
    "\n",
    "    def __init__(self, team_name: str, target_team: bool, lenc_bitlen: int) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        :param team_name: Team's name the network is created for.\n",
    "        :param target_team: Whether the team is in test set.\n",
    "        :param session: Current session used.\n",
    "        :param lenc_bitlen: Bitlength needed to encode all teams names.\n",
    "        \"\"\"\n",
    "        self._lenc_bitlen = lenc_bitlen\n",
    "        self._features = FEATURES_COMMON + FEATURES_WD\n",
    "        self._team_name = team_name\n",
    "        self._model = None\n",
    "        self._seed = _seed\n",
    "\n",
    "        # Store names of main head and head2 layers for direct access to layers\n",
    "        self._main_head_layers_names = []\n",
    "        self._main_head_stateful_layers_names = []\n",
    "        self._head2_layers_names = []\n",
    "        self._head2_stateful_layers_names = []\n",
    "\n",
    "        # Dropout and lr can differ for teams' models which are not used in testing\n",
    "        if target_team:\n",
    "            self._lr = LR\n",
    "            self._dropout = DROPOUT\n",
    "        else:\n",
    "            self._lr = NONTEST_LR\n",
    "            self._dropout = NONTEST_DROPOUT\n",
    "\n",
    "    def build(self) -> None:\n",
    "        self._model = self._assemble_network()\n",
    "\n",
    "\n",
    "    def _assemble_network(self) -> Model:\n",
    "        head1_inputs = []\n",
    "        head2_inputs = []\n",
    "\n",
    "        for f in self._features:\n",
    "            if f in FEATURES_TO_LENC:\n",
    "                head1_inputs.append(Input(batch_shape=(BATCH_SIZE, None, self._lenc_bitlen),\n",
    "                                          name=f\"input_team1_{f}\"))\n",
    "                head2_inputs.append(Input(batch_shape=(BATCH_SIZE, None, self._lenc_bitlen),\n",
    "                                          name=f\"input_team2_{f}\"))\n",
    "            else:\n",
    "                head1_inputs.append(Input(batch_shape=(BATCH_SIZE, None, 1), name=f\"input_team1_{f}\"))\n",
    "                head2_inputs.append(Input(batch_shape=(BATCH_SIZE, None, 1), name=f\"input_team2_{f}\"))\n",
    "\n",
    "        # Main head\n",
    "        head1_input_concat = concatenate(inputs=head1_inputs, name=\"head1_input_concat\")\n",
    "        head1_rnn1 = LSTM(UNITS,\n",
    "                          dropout=self._dropout,\n",
    "                          recurrent_dropout=0.5,\n",
    "                          stateful=STATEFUL,\n",
    "                          return_sequences=False,\n",
    "                          kernel_regularizer=l2(0.01),\n",
    "                          kernel_initializer=glorot_uniform(self._seed),\n",
    "                          name=\"head1_rnn1\")(head1_input_concat)\n",
    "        head1_fc1 = Dense(15,\n",
    "                          activation=\"elu\",\n",
    "                          kernel_regularizer=l2(0.01),\n",
    "                          kernel_initializer=glorot_uniform(self._seed),\n",
    "                          name=\"head1_fc1\")(head1_rnn1)\n",
    "\n",
    "        # Head2\n",
    "        head2_input_concat = concatenate(inputs=head2_inputs, name=\"head2_input_concat\")\n",
    "        head2_rnn1 = LSTM(UNITS,\n",
    "                          dropout=self._dropout,\n",
    "                          recurrent_dropout=0.5,\n",
    "                          stateful=STATEFUL,\n",
    "                          return_sequences=False,\n",
    "                          kernel_regularizer=l2(0.01),\n",
    "                          kernel_initializer=glorot_uniform(self._seed),\n",
    "                          name=\"head2_rnn1\")(head2_input_concat)\n",
    "        head2_fc1 = Dense(15,\n",
    "                          activation=\"elu\",\n",
    "                          kernel_regularizer=l2(0.01),\n",
    "                          kernel_initializer=glorot_uniform(self._seed),\n",
    "                          name=\"head2_fc1\")(head2_rnn1)\n",
    "\n",
    "        joint_concat = concatenate([head1_fc1, head2_fc1], name=\"joint_concat\")\n",
    "        output = Dense(2,\n",
    "                       activation=\"softmax\",\n",
    "                       kernel_initializer=glorot_uniform(self._seed),\n",
    "                       name=\"output\")(joint_concat)\n",
    "\n",
    "        model = Model(inputs=head1_inputs+head2_inputs,\n",
    "                      outputs=output,\n",
    "                      name=self._team_name)\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=self._lr, clipvalue=0.5, epsilon=1e-7),\n",
    "                      loss=SparseCategoricalCrossentropy(),\n",
    "                      metrics=[\"acc\"])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train_on_batch(self,\n",
    "                       x_input: Dict[str, np.ndarray],\n",
    "                       y_input: Dict[str, np.ndarray],\n",
    "                       ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        loss, acc = self._model.train_on_batch(x_input, y_input)\n",
    "        return loss, acc\n",
    "\n",
    "    def test_on_batch(self,\n",
    "                      x_input: Dict[str, np.ndarray],\n",
    "                      y_input: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \n",
    "        loss, acc = self._model.test_on_batch(x_input, y_input)\n",
    "        return loss, acc\n",
    "\n",
    "    def predict_on_batch(self, x_input: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        preds = self._model.predict_on_batch(x_input)\n",
    "            \n",
    "        return preds.flatten()\n",
    "\n",
    "    def decay_learning_rate(self) -> None:\n",
    "        current_lr = K.get_value(self._model.optimizer.lr)\n",
    "        K.set_value(self._model.optimizer.lr, current_lr * self._lrdecay)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### teams Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.enums import TargetVariable\n",
    "\n",
    "\n",
    "class SPModel:\n",
    "\n",
    "    def __init__(self, team_name: str, test_teams: List[str], lenc_bitlen: int) -> None:\n",
    "\n",
    "        self._team_name = team_name\n",
    "        self._lenc_bitlen = lenc_bitlen\n",
    "        self._target_variable = TargetVariable.FutureWD\n",
    "        self.__timesteps = _timesteps\n",
    "        self._features = FEATURES_COMMON + FEATURES_WD\n",
    "        self._target_team = team_name in test_teams\n",
    "\n",
    "        self.matches_data = {d: {\"idx\": 0, \"data\": {}} for d in Dataset}\n",
    "\n",
    "        tf.compat.v1.experimental.output_all_intermediates(True)\n",
    "\n",
    "        self.network = SPNetwork(self._team_name, self._target_team, lenc_bitlen)\n",
    "\n",
    "    def build_model(self) -> None:\n",
    "        self.network.build()\n",
    "\n",
    "    def train_on_batch(self,\n",
    "                       x_input: Dict[str, np.ndarray],\n",
    "                       y_input: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        return self.network.train_on_batch(x_input, y_input)\n",
    "\n",
    "    def test_on_batch(self,\n",
    "                      x_input: Dict[str, np.ndarray],\n",
    "                      y_input: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        return self.network.test_on_batch(x_input, y_input)\n",
    "\n",
    "    def predict_on_batch(self, x_input: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        return self.network.predict_on_batch(x_input)\n",
    "\n",
    "    def warm_up(self) -> None:\n",
    "        x_input, y_input = self.form_input(Dataset.Train, team2_model=self)\n",
    "        self.network.train_on_batch(x_input, y_input, self.class_weights)\n",
    "\n",
    "\n",
    "    def prepare_matches_data(self, dataset: Dataset, matches_data: pd.DataFrame) -> None:\n",
    "        i = 0\n",
    "\n",
    "        while True:\n",
    "            iend = i + self.__timesteps - 1\n",
    "            subset = matches_data.loc[i:iend]\n",
    "            if subset.empty:\n",
    "                break\n",
    "\n",
    "            # Default x, y values are none.\n",
    "            # If model encounters none values during training it will skip them\n",
    "            x_input = None\n",
    "            y_input = None\n",
    "\n",
    "            # Consider only chunks which length is equal to number of _timesteps and which are not the\n",
    "            # last chunk of data in the dataset (the last chunk is skipped to properly align match sequences)\n",
    "            if len(subset) == self.__timesteps and len(matches_data.loc[i:iend+1]) != self.__timesteps:\n",
    "                x_input = {}\n",
    "\n",
    "                # Reshape features\n",
    "                for f in self._features:\n",
    "                    # Teams names are stored as lists so they need to be stacked\n",
    "                    if f in FEATURES_TO_LENC:\n",
    "                        team1_names = np.vstack(subset.loc[:, f].values).reshape((1, -1, self._lenc_bitlen))\n",
    "                        x_input[f\"input_team1_{f}\"] = team1_names\n",
    "                    else:\n",
    "                        x_input[f\"input_team1_{f}\"] = subset.loc[:, f].values.reshape(1, -1, 1)\n",
    "\n",
    "                # Get target variable from last row and ignore it if it is none\n",
    "                y = subset.loc[iend, self._target_variable.value]\n",
    "                if y is not None and not np.isnan(y):\n",
    "                    y_input = {\"output\": y.reshape(-1, 1)}\n",
    "\n",
    "            self.matches_data[dataset][\"data\"][i] = {\"x_input\": x_input, \"y_input\": y_input}\n",
    "            i += 1\n",
    "\n",
    "    def form_input(self,\n",
    "                   dataset: Dataset,\n",
    "                   team2_model: \"SPModel\") -> Tuple[Optional[Dict[str, np.array]],\n",
    "                                                    Optional[Dict[str, np.array]]]:\n",
    "        # Get current data chunk based on index position for both models\n",
    "        i = self.matches_data[dataset][\"idx\"]\n",
    "        if i >= len(self.matches_data[dataset][\"data\"]):\n",
    "            return None, None\n",
    "        d1 = self.matches_data[dataset][\"data\"][i]\n",
    "        j = team2_model.matches_data[dataset][\"idx\"]\n",
    "        if j >= len(team2_model.matches_data[dataset][\"data\"]):\n",
    "            return None, None\n",
    "        d2 = team2_model.matches_data[dataset][\"data\"][j]\n",
    "\n",
    "        x_input = None\n",
    "        d2_as_team2 = {}\n",
    "\n",
    "        if d1[\"x_input\"] and d2[\"x_input\"]:\n",
    "            for k, v in d2[\"x_input\"].items():\n",
    "                d2_as_team2[k.replace(\"team1\", \"team2\")] = v\n",
    "\n",
    "            # Unpack both inputs into a single dict\n",
    "            x_input = {**d1[\"x_input\"], **d2_as_team2}\n",
    "\n",
    "        return x_input, d1[\"y_input\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "models: Dict[str, SPModel] = {}\n",
    "_teams_tuples = query_teams_ids_names_tuples()\n",
    "\n",
    "train_fixtures_ids = {}\n",
    "test_fixtures_ids = {}\n",
    "predict_fixtures_ids = {}\n",
    "train_teams = []\n",
    "test_teams = []\n",
    "predict_teams = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询fixtures数据\n",
    "fixturesData = query_fixtures_data(seasons)\n",
    "\n",
    "allTeams = get_unique_teams(fixturesData)\n",
    "last_season_teams = get_last_season_unique_teams(fixturesData)\n",
    "teams_fixtures_ids = {t: fixturesData[(fixturesData[\"home\"] == t) | (fixturesData[\"away\"] == t)].loc[:, \"id\"].tolist() for t in allTeams}\n",
    "\n",
    "# 检查缺失列\n",
    "_check_missing_columns(fixturesData)\n",
    "\n",
    "# 检查缺失值\n",
    "_check_nan_values(fixturesData, teams_fixtures_ids)\n",
    "\n",
    "# 划分训练、测试、预测fixtures id\n",
    "for t in last_season_teams:\n",
    "    predict_fixtures_ids[t] = teams_fixtures_ids[t][-NPREDICT:]\n",
    "    teams_fixtures_ids[t] = teams_fixtures_ids[t][:-NPREDICT]\n",
    "\n",
    "    test_fixtures_ids[t] = teams_fixtures_ids[t][-NTEST:]\n",
    "    teams_fixtures_ids[t] = teams_fixtures_ids[t][:-NTEST]\n",
    "\n",
    "train_fixtures_ids = teams_fixtures_ids\n",
    "\n",
    "# 划分训练集，测试集，预测集\n",
    "df_train, df_test, df_predict = _mask_out_dataset(fixturesData, train_fixtures_ids, test_fixtures_ids, predict_fixtures_ids)\n",
    "\n",
    "train_teams = get_unique_teams(df_train)\n",
    "test_teams = get_unique_teams(df_test)\n",
    "predict_teams = get_unique_teams(df_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据标准化\n",
    "fit_scalers(df_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fit_teams_names_lencoder() -> LabelEncoder:\n",
    "        df = query_teams_names()\n",
    "        values = [\"\"] + df[\"name\"].values.tolist()\n",
    "        return LabelEncoder().fit(values)  \n",
    "\n",
    "_teams_names_lenc = _fit_teams_names_lencoder()\n",
    "teams_names_bitlen = len(_teams_names_lenc.classes_).bit_length()\n",
    "\n",
    "for t in allTeams:\n",
    "    models[t] = SPModel(t, test_teams, teams_names_bitlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为每只球队准备数据\n",
    "for t in train_teams:\n",
    "    fixtures_ids = get_fixtures_ids_from_df(df_train, t)\n",
    "    team_matches_data = load_and_process_team_data(Dataset.Train, _teams_tuples[t], fixtures_ids)\n",
    "    models[t].prepare_matches_data(Dataset.Train, team_matches_data)\n",
    "\n",
    "for t in test_teams:\n",
    "    fixtures_ids = get_fixtures_ids_from_df(df_test, t)\n",
    "    aligned_fixtures_ids = align_fixtures_ids(df_train, t, fixtures_ids, _timesteps)\n",
    "    team_matches_data = load_and_process_team_data(Dataset.Test, _teams_tuples[t], aligned_fixtures_ids)\n",
    "    models[t].prepare_matches_data(Dataset.Test, team_matches_data)\n",
    "\n",
    "\n",
    "for t in predict_teams:\n",
    "    combined_df_train = pd.concat((df_train, df_test), ignore_index=True)\n",
    "    fixtures_ids = get_fixtures_ids_from_df(df_predict, t)\n",
    "    # Use combined train+test dataset in case that there would be less test samples than _timesteps\n",
    "    # so the rest of sequence can be filled from train dataset\n",
    "    aligned_fixtures_ids = align_fixtures_ids(combined_df_train, t, fixtures_ids, _timesteps)\n",
    "    team_matches_data = load_and_process_team_data(Dataset.Predict, _teams_tuples[t], aligned_fixtures_ids)\n",
    "    models[t].prepare_matches_data(Dataset.Predict, team_matches_data)    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble network for each model\n",
    "print(f\"Assembling {len(models)} models...\")\n",
    "\n",
    "for t in allTeams:\n",
    "    models[t].build_model()\n",
    "\n",
    "# Reset indices of dfs\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "df_predict.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_inputs = []\n",
    "test_y_inputs = []\n",
    "metics = []\n",
    "\n",
    "for epoch in range(1, _epochs):\n",
    "    for t in train_teams:\n",
    "        test_loss = []\n",
    "        test_acc = []\n",
    "        x = 0\n",
    "        for i, r in df_train[(df_train[\"home\"]==t) | (df_train[\"away\"]==t)].iterrows():\n",
    "            team1 = r[\"home\"]\n",
    "\n",
    "            if (_timesteps + x) >= len(df_train[(df_train[\"home\"]==team1) | (df_train[\"away\"]==team1)]):\n",
    "                x += 1\n",
    "                continue\n",
    "\n",
    "            # 查询这个数据集target对应的team2\n",
    "            targetFixture = df_train[(df_train[\"home\"]== team1) | (df_train[\"away\"]== team1)].iloc[_timesteps + x]\n",
    "            if targetFixture.loc[\"home\"] == team1:\n",
    "                team2 = targetFixture.loc[\"away\"]\n",
    "            else:\n",
    "                team2 = targetFixture.loc[\"home\"]\n",
    "\n",
    "\n",
    "            # Train home model\n",
    "            x_input, y_input = models[team1].form_input(Dataset.Train, models[team2])\n",
    "            loss, acc = models[team1].train_on_batch(x_input, y_input)\n",
    "\n",
    "            test_loss.append(loss)\n",
    "            test_acc.append(acc)\n",
    "\n",
    "        metics.append({\"team\": t, \"loss\": test_loss, \"acc\": test_acc})    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_no_metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
